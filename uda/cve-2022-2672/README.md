# CVE-2022-26721

A sanity check added to stop an integer-underflow (mapped_size < offset) caused an early `break` that left later descriptor slots uninitialized; cleanup blindly reads those slots and calls `munmap` on attacker-controlled garbage (UDA → unmap abuse).

## Context / ACID
- XPC = macOS IPC; guest-ish attacker (web content → Safari) can send arrays of TLV descriptors to a privileged daemon (CVMServer). those descriptors are ACID.  
- CVMServer maps shared memory (`xpc_shmem_map`) into its address space; mapped addresses/sizes become ACID handles the process trusts.  
- Code allocates a flat `descriptors` array (4×count) to hold per-item fields; later cleanup blindly uses those slots for `munmap`.

## First thoughts
Looked for the underflow math (offset/size/mapped_size) — that was the original bug. patch added a sanity `if (mapped_size < offset) break;` which seemed sane, but breaking early smells like it could leave state half-initialized.

## Actual trace (short)
- `count` = attacker-controlled (SACI). `descriptors = malloc(4 * count * sizeof(size_t))` (non-zeroed).  
- Inside the loop they *initialize elements to 0 then* process the chunk.  
- If `mapped_size < offset` check hits, code `break`s out of the loop early.  
- Cleanup later iterates `index = 0..count-1` and does `if (mappedLength[index]) munmap(mappedBaseAddress[index], mappedLength[index]);`  
- For indexes past the `break` point the `descriptors` slots were never touched (malloc garbage).  
- Heap feng shui can make those garbage words attacker-controlled → munmap on attacker-chosen addresses → attacker unmaps useful regions and later reclaims them → control flow corruption.

## Root cause
Initialization (zeroing) happens *per-iteration*, not atomically for the whole allocation. An early `break` leaves tail slots uninitialized; cleanup assumes every slot is safe to read.

## Exploitability
UDA by itself may crash. Real exploit path: attacker grooms heap so malloc returns memory pre-filled with chosen pointers → trigger early break → cleanup munmaps attacker-chosen addresses → attacker reclaims/unmaps targets (function pointers, vtables, etc.) → place controlled data → control flow hijack. Requires heap grooming + timing but practical in this service model (CVMServer accessible from sandboxed browsers, runs as root).

## Patch
Use zeroed allocation or ensure full initialization before any early-escape:
- `descriptors = calloc(4 * count, sizeof(size_t))` (or `g_malloc0`) — simplest fix.
- OR move the init to a single `memset(descriptors, 0, ...)` before the loop.
- OR track `initialized_count` and only cleanup up to that number.

## Takeaway
If you allocate an array of handles, zero it up-front (or track what you initialized). early exits + per-iteration init = UDA waiting to happen.

